{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental NN-library for Python 3\n",
    "The following classes can be used to create a simple multilayer feed-forward neural network using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class NeuralNetwork:    \n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, lr, activation='sigmoid'):\n",
    "        self.n_outputs = n_outputs\n",
    "        self.lr = lr\n",
    "        self.activation = activation\n",
    "        self.layers = []\n",
    "        # input layer\n",
    "        self.layers.append(Layer(n_inputs, n_hidden[0], lr, activation))\n",
    "        # hidden layers\n",
    "        for i in range(len(n_hidden)-1):\n",
    "            self.layers.append(Layer(n_hidden[i], n_hidden[i+1], lr, activation))\n",
    "        # output layer\n",
    "        self.layers.append(Layer(n_hidden[-1], n_outputs, lr, activation))\n",
    "        \n",
    "    def feed_forward(self, X):\n",
    "        self.layers[0].activate(X)\n",
    "        for i in range(1,len(self.layers)):\n",
    "            self.layers[i].activate(self.layers[i-1].outputs)\n",
    "        return self.layers[-1].outputs\n",
    "    \n",
    "    def back_propagate(self, X, y):\n",
    "        # compute output layer gamma\n",
    "        nn_output = self.feed_forward(X)\n",
    "        error = nn_output - y\n",
    "        gammas = error * self.activation_prime(nn_output)\n",
    "        \n",
    "        layer_inputs = self.layers[-2].outputs\n",
    "        self.layers[-1].update_weights(layer_inputs, gammas, self.lr)\n",
    "        \n",
    "        # calculate hidden layer gammas\n",
    "        for i in range(2, len(self.layers)+1):\n",
    "            if i == len(self.layers):\n",
    "                layer_inputs = X            \n",
    "            else: layer_inputs = self.layers[-i-1].outputs\n",
    "            layer_outputs = self.layers[-i].outputs\n",
    "            weights = np.transpose(self.layers[-i+1].weights)\n",
    "            gammas = np.sum(gammas*weights, axis=1) * self.activation_prime(layer_outputs)\n",
    "            self.layers[-i].update_weights(layer_inputs, gammas, self.lr)   \n",
    "            \n",
    "    def train(self, X, y, convert=False, epochs=100, display_step=10):\n",
    "        if convert:\n",
    "            y = self.convert_targets(y)\n",
    "        for epoch in range(epochs):\n",
    "            error = 0.\n",
    "            for i in range(len(X)):\n",
    "                batch_X = X[i]\n",
    "                batch_y = y[i]\n",
    "                if self.n_outputs != len(batch_y):\n",
    "                    print('Wrong dimensions of target list!')\n",
    "                self.back_propagate(batch_X, batch_y)\n",
    "                error += self.compute_error(batch_X, batch_y)\n",
    "            \n",
    "            mse = (1/len(X)) * error\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                    print('Completed epoch', '%d' % (epoch+1), '/ %d' % epochs,\n",
    "                        'Error', '%.5f' % mse)\n",
    "                \n",
    "\n",
    "    def activation_prime(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return x*(1.0-x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return 1.0-(x*x)        \n",
    "    \n",
    "    def softmax(self, w, t = 1.0):\n",
    "        e = np.exp(np.array(w) / t)\n",
    "        dist = e / np.sum(e)\n",
    "        return dist\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred = self.feed_forward(X)\n",
    "        return pred.argmax()\n",
    "        \n",
    "    def predict_prob(self, X):\n",
    "        return self.softmax(self.feed_forward(X))\n",
    "    \n",
    "    def accuracy(self, X, y):  \n",
    "        acc = 0.\n",
    "        for i in range(len(y)):\n",
    "            if y[i] == nn.predict(X[i]):\n",
    "                acc += 1\n",
    "        return acc / len(y)\n",
    "    \n",
    "    def convert_targets(self, targets):        \n",
    "        enc = preprocessing.LabelEncoder()\n",
    "        enc.fit(targets)\n",
    "        conv_targets = []\n",
    "        n_classes = len(enc.classes_)\n",
    "        \n",
    "        conv_targets = []\n",
    "        for target in targets:\n",
    "            cnt = 0\n",
    "            for cl in enc.classes_:\n",
    "                if target == enc.classes_[cnt]:\n",
    "                    arr = np.zeros(n_classes)\n",
    "                    arr[cnt] = 1\n",
    "                    conv_targets.append(arr)\n",
    "                cnt += 1\n",
    "        return conv_targets\n",
    "      \n",
    "    def compute_error(self, X, y):\n",
    "        # X/y must be value pair for one training sample\n",
    "        nn_outputs = self.feed_forward(X)        \n",
    "        error = (1/len(X)) * sum(np.square(nn_outputs - y))\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_inputs, n_outputs, lr, activation):\n",
    "        self.lr = lr\n",
    "        self.activation = activation\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "        for i in range(n_outputs):\n",
    "            self.weights.append([np.random.randn() for _ in range(n_inputs)])\n",
    "            self.bias.append(np.random.randn())\n",
    "            \n",
    "    def activation_fn(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return 1.0/(1.0+np.exp(-x))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "                            \n",
    "    def activate(self, inputs):\n",
    "        outputs = []\n",
    "        for i in range(self.n_outputs): \n",
    "            # multiply weights by inputs\n",
    "            outputs.append(sum(np.multiply(self.weights[i], inputs)))\n",
    "        # add bias and apply activation function\n",
    "        self.outputs = np.add(outputs, self.bias)\n",
    "        self.outputs = self.activation_fn(self.outputs)\n",
    "        return outputs\n",
    "    \n",
    "    def update_weights(self, inputs, gammas, lr):\n",
    "        for o in range(self.n_outputs):\n",
    "            for i in range(len(self.weights[o])):\n",
    "                self.weights[o][i] -= lr * gammas[o] * inputs[i]                \n",
    "                self.bias[o] -= lr * gammas[o]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "In the following example the data will be split up into a training set and a test set using sklearn. I'll use the well-known Iris dataset which can be retrieved directly from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(data, labels, test_size=0.3)\n",
    "train_y = np.reshape(train_y, (len(train_y),1))\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.fit_transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you'll use the constructor of the NeuralNetwork class to create a new network.\n",
    "You must specify the number of input neurons, a list containing the number of neurons for a variable amount of hidden layers, the number of output neurons, the learning rate, as well as the activation function (sigmoid or tanh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(4, [2,3], 3, lr=0.08, activation='tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the NeuralNetwork.train() function to train the network.\n",
    "The function takes the inputs and target lists as parameters, an optional bool parameter to specify whether to convert the input list into a onehot-list, the number of iterations to train the network (epochs), as well a display_step parameter for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programme\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed epoch 20 / 200 Error 0.00564\n",
      "Completed epoch 40 / 200 Error 0.00487\n",
      "Completed epoch 60 / 200 Error 0.00446\n",
      "Completed epoch 80 / 200 Error 0.00366\n",
      "Completed epoch 100 / 200 Error 0.00281\n",
      "Completed epoch 120 / 200 Error 0.00228\n",
      "Completed epoch 140 / 200 Error 0.00244\n",
      "Completed epoch 160 / 200 Error 0.00307\n",
      "Completed epoch 180 / 200 Error 0.00363\n",
      "Completed epoch 200 / 200 Error 0.00394\n"
     ]
    }
   ],
   "source": [
    "nn.train(train_X, train_y, convert=True, epochs=200, display_step=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the accuracy of the model on the dataset, use the NeuralNetwork.accuracy() function. This function requires the inputs and targets of the respective dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9714\n",
      "Test accuracy: 0.9778\n"      
     ]
    }
   ],
   "source": [    
    "print('Train accuracy:', '%.4f' % nn.accuracy(train_X, train_y))\n",
    "print('Test accuracy:', '%.4f' % nn.accuracy(test_X, test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use NeuralNetwork.predict() to get the predicted value for one element of the test set. A return value of zero means the first neuron in the output layer has been activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output: 2\n",
      "Target output: 2\n"
     ]
    }
   ],
   "source": [
    "print('Predicted output:', nn.predict(test_y[0]))\n",
    "print('Target output:', test_y[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
