{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class NeuralNetwork:    \n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, lr, activation='relu'):\n",
    "        self.n_outputs = n_outputs\n",
    "        self.lr = lr\n",
    "        self.activation = activation\n",
    "        self.layers = []\n",
    "        # input layer\n",
    "        self.layers.append(Layer(n_inputs, n_hidden[0], lr, activation))\n",
    "        # hidden layers\n",
    "        for i in range(len(n_hidden)-1):\n",
    "            self.layers.append(Layer(n_hidden[i], n_hidden[i+1], lr, activation))\n",
    "        # output layer\n",
    "        self.layers.append(Layer(n_hidden[-1], n_outputs, lr, activation))\n",
    "        \n",
    "    def feed_forward(self, X):\n",
    "        self.layers[0].activate(X)\n",
    "        for i in range(1,len(self.layers)):\n",
    "            self.layers[i].activate(self.layers[i-1].outputs)\n",
    "        return self.layers[-1].outputs\n",
    "    \n",
    "    def back_propagate(self, X, y):\n",
    "        # compute output layer gamma\n",
    "        nn_output = self.feed_forward(X)\n",
    "        error = nn_output - y\n",
    "        gammas = error * self.activation_prime(nn_output)\n",
    "        \n",
    "        layer_inputs = self.layers[-2].outputs\n",
    "        self.layers[-1].update_weights(layer_inputs, gammas, self.lr)\n",
    "        \n",
    "        #calculate hidden layer gammas\n",
    "        for i in range(2, len(self.layers)+1):\n",
    "            if i == len(self.layers):\n",
    "                layer_inputs = X            \n",
    "            else: layer_inputs = self.layers[-i-1].outputs\n",
    "            layer_outputs = self.layers[-i].outputs\n",
    "            weights = np.transpose(self.layers[-i+1].weights)\n",
    "            gammas = np.sum(gammas*weights, axis=1) * self.activation_prime(layer_outputs)\n",
    "            self.layers[-i].update_weights(layer_inputs, gammas, self.lr)   \n",
    "            \n",
    "    def train(self, X, y, convert=False, epochs=100, display_step=10):\n",
    "        if convert:\n",
    "            y = self.convert_targets(y)\n",
    "        for epoch in range(epochs):\n",
    "            error = 0.\n",
    "            for i in range(len(X)):\n",
    "                batch_X = X[i]\n",
    "                batch_y = y[i]\n",
    "                if self.n_outputs != len(batch_y):\n",
    "                    print('Wrong dimensions of target list!')\n",
    "                self.back_propagate(batch_X, batch_y)\n",
    "                error += self.compute_error(batch_X, batch_y)\n",
    "            \n",
    "            mse = (1/len(X)) * error\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                    print('Completed epoch', '%d' % (epoch+1), '/ %d' % epochs,\n",
    "                        'Error', '%.5f' % mse)\n",
    "                \n",
    "\n",
    "    def activation_prime(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return x*(1.0-x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return 1.0-(x*x)\n",
    "        elif self.activation == 'relu':\n",
    "            return np.heaviside(x,0)\n",
    "    \n",
    "    def softmax(self, w, t = 1.0):\n",
    "        e = np.exp(np.array(w) / t)\n",
    "        dist = e / np.sum(e)\n",
    "        return dist\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred = self.feed_forward(X)\n",
    "        return pred.argmax()\n",
    "        \n",
    "    def predict_prob(self, X):\n",
    "        return self.softmax(self.feed_forward(X))\n",
    "    \n",
    "    def convert_targets(self, targets):        \n",
    "        enc = preprocessing.LabelEncoder()\n",
    "        enc.fit(targets)\n",
    "        conv_targets = []\n",
    "        n_classes = len(enc.classes_)\n",
    "        \n",
    "        conv_targets = []\n",
    "        for target in targets:\n",
    "            cnt = 0\n",
    "            for cl in enc.classes_:\n",
    "                if target == enc.classes_[cnt]:\n",
    "                    arr = np.zeros(n_classes)\n",
    "                    arr[cnt] = 1\n",
    "                    conv_targets.append(arr)\n",
    "                cnt += 1\n",
    "        return conv_targets\n",
    "      \n",
    "    def compute_error(self, X, y):\n",
    "        # X/y must be value pair for one training sample\n",
    "        nn_outputs = self.feed_forward(X)\n",
    "        # element wise error for each output neuron -->\n",
    "        # square it --> add them --> divide by number of samples\n",
    "        error = (1/len(X)) * sum(np.square(nn_outputs - y))\n",
    "        return error\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, n_inputs, n_outputs, lr, activation):\n",
    "        self.lr = lr\n",
    "        self.activation = activation\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "        for i in range(n_outputs):\n",
    "            self.weights.append([np.random.randn() for _ in range(n_inputs)])\n",
    "            self.bias.append(np.random.randn())\n",
    "            \n",
    "    def activation_fn(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return 1.0/(1.0+np.exp(-x))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(x,0)\n",
    "                            \n",
    "    def activate(self, inputs):\n",
    "        outputs = []\n",
    "        for i in range(self.n_outputs): \n",
    "            # multiply weights by inputs\n",
    "            outputs.append(sum(np.multiply(self.weights[i], inputs)))\n",
    "        # add bias and apply activation function\n",
    "        self.outputs = np.add(outputs, self.bias)\n",
    "        self.outputs = self.activation_fn(self.outputs)\n",
    "        return outputs\n",
    "    \n",
    "    def update_weights(self, inputs, gammas, lr):\n",
    "        for o in range(self.n_outputs):\n",
    "            for i in range(len(self.weights[o])):\n",
    "                self.weights[o][i] -= lr * gammas[o] * inputs[i]                \n",
    "                self.bias[o] -= lr * gammas[o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programme\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed epoch 20 / 200 Error 0.12686\n",
      "Completed epoch 40 / 200 Error 0.08687\n",
      "Completed epoch 60 / 200 Error 0.07773\n",
      "Completed epoch 80 / 200 Error 0.07115\n",
      "Completed epoch 100 / 200 Error 0.06561\n",
      "Completed epoch 120 / 200 Error 0.06120\n",
      "Completed epoch 140 / 200 Error 0.05575\n",
      "Completed epoch 160 / 200 Error 0.04463\n",
      "Completed epoch 180 / 200 Error 0.02991\n",
      "Completed epoch 200 / 200 Error 0.01937\n",
      "Test accuracy: 0.9778\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(data, labels, test_size=0.3)\n",
    "train_y = np.reshape(train_y, (len(train_y),1))\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.fit_transform(test_X)\n",
    "\n",
    "nn = NeuralNetwork(4,[2,3],3, lr=0.08, activation='sigmoid')\n",
    "nn.train(train_X, train_y, convert=True, epochs=200, display_step=20)\n",
    "\n",
    "acc = 0.\n",
    "for j in range(len(test_y)):\n",
    "    if test_y[j] == nn.predict(test_X[j]):\n",
    "        acc += 1\n",
    "print('Test accuracy:', '%.4f' % (acc/len(test_y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
